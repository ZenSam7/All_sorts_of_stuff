{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "import tensorflow as tf\n",
    "import pathlib"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T18:24:02.885504Z",
     "start_time": "2024-04-09T18:24:02.431534100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создаём датасет с цветочками"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# (лучше загрузить свой с инета)\n",
    "data_dir = tf.keras.utils.get_file(origin=\"C:\\\\Users\\\\samki\\\\Downloads\\\\flowers.zip\",\n",
    "                                   fname=\"flower_photos\",\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "# Константы\n",
    "batch_size = 32\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "\n",
    "# Разбиваем датасет на тренировочную группу и группу валидации\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=5432,\n",
    ")\n",
    "\n",
    "validation_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=5432,\n",
    ")\n",
    "\n",
    "\n",
    "train_data = train_data.map(lambda x, y: (x / 255.0, x / 255.0))\n",
    "validation_data = validation_data.map(lambda x, y: (x / 255.0, x / 255.0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T18:28:24.903069700Z",
     "start_time": "2024-04-09T18:28:24.538976200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 8)                 2887668   \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 150, 150, 3)       130451    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,018,119\n",
      "Trainable params: 3,018,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Reshape, Dense, Cropping2D\n",
    "\n",
    "# Модель\n",
    "input_shape = (img_height, img_width, 3)\n",
    "latent_dim = 8\n",
    "\n",
    "# Кодировщик\n",
    "encoder_inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "x = Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(8, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(4, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*4, activation=\"relu\")(x)\n",
    "x = Dense(latent_dim*2, activation=\"relu\")(x)\n",
    "latent_vector = Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "encoder = keras.Model(encoder_inputs, latent_vector, name=\"encoder\")\n",
    "\n",
    "# Декодировщик\n",
    "latent_inputs = Input(shape=(latent_dim,))\n",
    "x = Dense(20 * 20 * 32, activation=\"sigmoid\")(latent_inputs)\n",
    "x = Reshape((20, 20, 32))(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = Conv2DTranspose(16, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = Conv2DTranspose(8, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = Conv2DTranspose(3, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder_outputs = Cropping2D((5, 5))(decoder_outputs)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# Автоэнкодер\n",
    "autoencoder_inputs = Input(shape=input_shape)\n",
    "latent_vector = encoder(autoencoder_inputs)\n",
    "autoencoder_outputs = decoder(latent_vector)\n",
    "autoencoder = keras.Model(autoencoder_inputs, autoencoder_outputs, name=\"autoencoder\")\n",
    "autoencoder.compile(loss=\"mae\", optimizer=\"rmsprop\")\n",
    "\n",
    "autoencoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T18:29:16.884751900Z",
     "start_time": "2024-04-09T18:29:16.755738100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 0.2432 - val_loss: 0.2393\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2376 - val_loss: 0.2351\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 0.2342 - val_loss: 0.2306\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2262 - val_loss: 0.2238\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2113 - val_loss: 0.2009\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2049 - val_loss: 0.2020\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2020 - val_loss: 0.1992\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.2000 - val_loss: 0.1956\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1967 - val_loss: 0.1895\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1940 - val_loss: 0.1921\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1919 - val_loss: 0.1862\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1900 - val_loss: 0.1916\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1892 - val_loss: 0.2056\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1875 - val_loss: 0.1849\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1861 - val_loss: 0.1830\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1848 - val_loss: 0.1782\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1843 - val_loss: 0.1784\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1836 - val_loss: 0.1828\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1825 - val_loss: 0.1797\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 0.1827 - val_loss: 0.1792\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x192c06b9ea0>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(\n",
    "    train_data,\n",
    "    epochs=20,\n",
    "    validation_data=validation_data,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T18:32:47.992574300Z",
     "start_time": "2024-04-09T18:29:17.662979500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 30ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'OwnedIterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_images):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# Исходное изображение\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m2\u001B[39m, num_images, i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m     plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     13\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOriginal\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m     plt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'OwnedIterator' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Генерация изображений с помощью автоэнкодера\n",
    "generated_images = autoencoder.predict(validation_data)\n",
    "num_images = 10\n",
    "\n",
    "# Отображение сгенерированных изображений\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(num_images):\n",
    "    # Исходное изображение\n",
    "    plt.subplot(2, num_images, i + 1)\n",
    "    plt.imshow(validation_data[i])\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Сгенерированное изображение\n",
    "    plt.subplot(2, num_images, i + num_images + 1)\n",
    "    plt.imshow(generated_images[i])\n",
    "    plt.title('Generated')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Пример использования\n",
    "plot_generated_images(autoencoder, validation_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T18:33:33.305042700Z",
     "start_time": "2024-04-09T18:33:32.398042200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
